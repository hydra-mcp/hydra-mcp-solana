# HYDRA-AI Frontend

HYDRA-AI is a modern AI chat application built with React, TypeScript, and Tailwind CSS. The project utilizes the latest web technology stack to provide a smooth, responsive user experience for blockchain project analysis and AI assistant interactions.

## Features

- ğŸ” User authentication system with token-based authorization and refresh functionality
- ğŸ’¬ Real-time AI chat functionality with SSE (Server-Sent Events) streaming responses
- ğŸ” Blockchain project analysis capabilities through specialized AI assistants
- ğŸ“Š Multi-stage processing visualization for complex analysis tasks
- ğŸ’¾ Local chat history management with persistence
- ğŸ”” Toast notifications and error handling system
- ğŸ¨ Modern UI using Tailwind CSS and Shadcn components
- ğŸ“± Responsive design with specialized layouts for different devices (including iOS)
- ğŸš€ Built with Vite, providing a fast development experience
- ğŸ‘› Wallet connectivity features for blockchain interactions

## Tech Stack

- **Framework**: React 18
- **Routing**: React Router v7
- **State Management**: React Context API
- **Styling**: Tailwind CSS
- **UI Components**: Radix UI / Shadcn
- **Build Tool**: Vite
- **Package Manager**: pnpm
- **Language**: TypeScript
- **Animation**: Framer Motion
- **API Communication**: Fetch API with SSE support

## Development Environment Setup

### Prerequisites

- Node.js (Recommended v18+)
- pnpm (v10+)

### Installation

1. Clone the repository

```bash
git clone https://github.com/hydra-mcp/hydra-mcp-solana.git
cd hydra-mcp-solana
```

2. Install dependencies

```bash
pnpm install
```

3. Configure environment variables

Create a `.env.local` file (or edit the existing one):

```
VITE_API_BASE_URL=your_api_endpoint
```

4. Start the development server

```bash
pnpm dev
```

The application will run on [http://localhost:5173](http://localhost:5173).

## Application Structure

The application is organized into the following key components:

### Core Components

- **Chat Interface**: A full-featured chat UI with message history, streaming responses, and context management
- **Authentication System**: Login page with token-based authentication
- **Wallet Integration**: Connection to blockchain wallets for crypto interactions
- **Error Handling**: Global error boundary and API error handling system

### Pages

- **Home**: Landing page for the application
- **ChatPage**: Main chat interface with AI assistant
- **WalletFinder**: Interface for connecting to blockchain wallets
- **IOSDesktop**: Specialized interface for iOS devices

## Build and Deployment

### Production Build

```bash
pnpm build
```

The built files will be located in the `dist` directory.

### Deployment Process

#### Using Caddy Server (Recommended)

1. Install Caddy Server
   
   Please refer to the [Caddy official documentation](https://caddyserver.com/docs/install) for installation.

2. Configure Caddyfile

   Create or edit the Caddyfile:

   ```
   your-domain.com {
     root * /path/to/hydra-front/dist
     
     # Set up SPA routing
     try_files {path} {path}/ /index.html
     
     # Define static resource matcher
     @static {
       path *.css *.js *.ico *.gif *.jpg *.jpeg *.png *.svg *.webp *.woff *.woff2
     }
     
     # Static resource cache settings
     header @static Cache-Control "public, max-age=31536000, immutable"
     
     # HTML file cache settings
     @html {
       path *.html
     }
     header @html Cache-Control "no-cache, no-store, must-revalidate"
     
     # API proxy settings (if needed)
     reverse_proxy /api/* your_backend_api_server
     
     # Enable file server
     file_server
   }
   ```

3. Start Caddy Server

   ```bash
   caddy run
   ```

#### Using Docker Deployment

1. Use Dockerfile

   The project already includes a Dockerfile, which can be built directly:

   ```bash
   docker build -t hydra-front .
   docker run -d -p 80:80 hydra-front
   ```

2. Use docker-compose

   The project provides a docker-compose.yml file, which can be used to deploy both the frontend and backend:

   ```bash
   # Start the service
   docker-compose up -d
   
   # View logs
   docker-compose logs -f
   
   # Stop the service
   docker-compose down
   ```

   Note: Please adjust the configuration in docker-compose.yml according to your actual situation before using it.

## Environment Variables

- `VITE_API_BASE_URL`: API server base URL
- `VITE_BASE_URL`: Optional alternative API base URL (for development/testing)

## Project Structure

```
src/
â”œâ”€â”€ components/        # Reusable UI components
â”‚   â”œâ”€â”€ chat/          # Chat-related components
â”‚   â”œâ”€â”€ ui/            # Core UI components (Shadcn)
â”‚   â”œâ”€â”€ phantom/       # Wallet connection components
â”‚   â””â”€â”€ ios/           # iOS-specific components
â”œâ”€â”€ contexts/          # React contexts for state management
â”œâ”€â”€ hooks/             # Custom React hooks
â”œâ”€â”€ layouts/           # Layout components
â”œâ”€â”€ lib/               # Utility functions and API clients
â”‚   â”œâ”€â”€ api.ts         # API communication layer
â”‚   â”œâ”€â”€ sse.ts         # Server-Sent Events implementation
â”‚   â””â”€â”€ utils.ts       # General utility functions
â”œâ”€â”€ pages/             # Application pages
â”œâ”€â”€ types/             # TypeScript type definitions
â”œâ”€â”€ App.tsx            # Main application component with routes
â”œâ”€â”€ Login.tsx          # Authentication page
â”œâ”€â”€ index.css          # Global styles
â””â”€â”€ main.tsx           # Application entry point
```

## API Documentation

HYDRA-AI frontend uses the `/agent/chat/completions` API endpoint to interact with the AI assistant, implementing blockchain project analysis functionality. This API is similar to the structure of OpenAI's Chat Completions API, suitable for frontend developers familiar with LLM APIs. For the complete API documentation, please refer to [API documentation](API.md).

### Endpoint

```
POST /agent/chat/completions
```

### Authentication
- Requires an authenticated user session
- Uses JWT authentication (managed by the `get_current_active_user` dependency)

### Request Format

```json
{
  "model": "gpt-4",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Analyze the project at address 0x123..."
    }
  ],
  "stream": true,
  "temperature": 0.7,
  "max_tokens": 1024,
  "project_context": {
    "additional_context": "any relevant context"
  }
}
```

### Response Types

API provides two response modes:
1. **Non-streaming response** - Full response returned once
2. **Streaming response** - Returned in Server-Sent Events (SSE) format, containing the following event types:
   - **Stage event** - Represents different stages of the analysis process
   - **Content event** - Passes actual content blocks
   - **Error event** - Passes error information
   - **Done event** - Represents the end of the stream

## Contribution

Welcome to contribute! Please follow the following steps:

1. Fork the repository
2. Create your feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add some amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## License

[Apache 2.0](LICENSE) 